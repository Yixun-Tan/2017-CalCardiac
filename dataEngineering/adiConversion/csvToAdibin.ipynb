{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import struct\n",
    "    , csv\n",
    "    , json\n",
    "    , base64\n",
    "    , zlib\n",
    "    , pprint\n",
    "    , numpy\n",
    "    , ctypes\n",
    "    , glob\n",
    "    , os\n",
    "    , sys\n",
    "    , time\n",
    "    , sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Relevant Python Documentation for Reading C Structs into Python](https://docs.python.org/3.5/library/struct.html \"Python Docs for C Structs\")\n",
    "\n",
    "#### [Excellent code snippets for working with Binary data in Python](https://www.devdungeon.com/content/working-binary-data-python \"DevDungeon\")\n",
    "\n",
    "|doc | url |\n",
    "|-----|-----|\n",
    "| adiBin docs| http://cdn.adinstruments.com/adi-web/manuals/translatebinary/LabChartBinaryFormat.pdf |\n",
    "| adiBin header|  http://cdn.adinstruments.com/adi-web/manuals/translatebinary/ADIBinaryFormat.h |\n",
    "| adiBin example|http://cdn.adinstruments.com/adi-web/manuals/translatebinary/TranslateBinary.c |\n",
    "|source | https://forum.adinstruments.com/viewtopic.php?t=395 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base64 to JSON function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base64ToJson(zippedString):\n",
    "    json_str = zlib.decompress(base64.b64decode(zippedString)).decode()\n",
    "    json_json = json.loads(json_str)\n",
    "    return json_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse CSV Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parseCsv(csv_row, dbg=False):\n",
    "    \n",
    "    ############################################################################\n",
    "    # Parse CSV Data\n",
    "    ############################################################################\n",
    "    \n",
    "    # Record Identifiers \n",
    "    alarm_id = csv_row[0]\n",
    "    time_since_admission = csv_row[1]\n",
    "    \n",
    "    # Decompress channel data\n",
    "    channel_json = base64ToJson(csv_row[2])\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # Parse file header and channel header data\n",
    "    ############################################################################\n",
    "    \n",
    "    # Number of channels\n",
    "    num_channels = len(channel_json)\n",
    "    \n",
    "    # Channel titles\n",
    "    channel_titles = []\n",
    "    for i in range(num_channels):\n",
    "        channel_titles.append(channel_json[i]['Label'].upper())\n",
    "        \n",
    "    \n",
    "\n",
    "    ############################################################################\n",
    "    # Parse channel data\n",
    "    ############################################################################\n",
    "    \n",
    "    # Convert channel data to list\n",
    "    channel_data = []\n",
    "    for i in range(num_channels):\n",
    "        channel_data.append([int(j) for j in channel_json[i]['Text'].split(',')])\n",
    "        \n",
    "    \n",
    "    # Check to see if all channels have the same length\n",
    "    # If channels are the same length, samples_per_channel set to len element 0\n",
    "    # If theres a mismatch, pad other channels with mean and\n",
    "    #   set samples_per_channel to max length\n",
    "    \n",
    "    if all(len(i) == len(channel_data[0]) for i in channel_data):\n",
    "        samples_per_channel = len(channel_data[0])\n",
    "        if dbg == True:\n",
    "            print(\"all channel_data sublists equal length\")\n",
    "    else:\n",
    "        max_sublist_len = len(max(channel_data, key=len))\n",
    "        for i in range (num_channels):\n",
    "            len_dif_tuple = (0, max_sublist_len - len(channel_data[i]))\n",
    "            channel_data[i] = numpy.pad(channel_data[i]\\\n",
    "                                        , pad_width = len_dif_tuple\\\n",
    "                                        , mode='mean'\n",
    "                                       )\n",
    "        samples_per_channel = max_sublist_len  \n",
    "        if dbg == True:\n",
    "            print(\"all channel_data sublists NOT equal length \\\n",
    "                  - padded w/ trailing means\")\n",
    "\n",
    "\n",
    "    ############################################################################\n",
    "    # Create dictionary of all parsed data from csv file\n",
    "    ############################################################################\n",
    "    \n",
    "    csv_data = {\n",
    "          'alarm_id' : alarm_id\n",
    "        , 'time_since_admission' : time_since_admission\n",
    "        , 'num_channels' : num_channels\n",
    "        , 'channel_titles' : channel_titles\n",
    "        , 'channel_data' : channel_data\n",
    "        , 'samples_per_channel' : samples_per_channel\n",
    "    }\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # Debug: Check values\n",
    "    ############################################################################\n",
    "    if dbg == True:\n",
    "        print(\"alarm_id:\", alarm_id)\n",
    "        print(\"time_since_admission:\", time_since_admission)\n",
    "        print(\"num_channels:\", num_channels)\n",
    "        print(\"channel_titles:\", channel_titles)\n",
    "        print(\"samples_per_channel:\", samples_per_channel)\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    \n",
    "    ############################################################################\n",
    "    # Return\n",
    "    ############################################################################\n",
    "    return csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create ADIBIN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createAdibin(data_dict, dbg=False):\n",
    "       \n",
    "    ############################################################################\n",
    "    # Define default adibin file header variables\n",
    "    ############################################################################\n",
    "    \n",
    "    # Define default file size variables for writing adibin file\n",
    "    FILE_HEADER_LENGTH = 68\n",
    "    CHANNEL_HEADER_LENGTH = 96\n",
    "    # CHANNEL_TITLE_LENGTH = 32\n",
    "    # UNITS_LENGTH = 32\n",
    "    \n",
    "    # Define format strings for struct\n",
    "    ADI_FILE_HEADER_FORMAT_STRING = \"<4sldlllllddllll\"\n",
    "    ADI_CHANNEL_HEADER_FORMAT_STRING = \"<32s32sdddd\"\n",
    "    ADI_CHANNEL_DATA_FORMAT_STRING = \"<h\"\n",
    "    \n",
    "    # Define default file header variables\n",
    "    magic = b'CFWB'\n",
    "    version = 1\n",
    "    secs_per_tick = 1/240\n",
    "    year = 1776\n",
    "    month = 7\n",
    "    day = 4\n",
    "    hour = 0\n",
    "    minute = 0\n",
    "    second = 0\n",
    "    trigger = 0\n",
    "    # num_channels passed in dict\n",
    "    # samples_per_channel passed in dict\n",
    "    time_channel = 0\n",
    "    data_format = 3\n",
    "    \n",
    "    # Define default channel header variables\n",
    "    # channel_title passed in dict\n",
    "    units = {'I':b'mV' \\\n",
    "             , 'II':b'mV' \\\n",
    "             , 'III':b'mV' \\\n",
    "             , 'V':b'mV' \\\n",
    "             , 'AVR':b'mV' \\\n",
    "             , 'AVL':b'mV' \\\n",
    "             , 'AVF':b'mV'  \\\n",
    "             , 'AR1':b'mmHg' \\\n",
    "             , 'AR2':b'mmHg' \\\n",
    "             , 'AR3':b'mmHg' \\\n",
    "             , 'AR4':b'mmHg' \\\n",
    "             , 'AR5':b'mmHg' \\\n",
    "             , 'AR6':b'mmHg' \\\n",
    "             , 'AR7':b'mmHg' \\\n",
    "             , 'AR8':b'mmHg' \\\n",
    "             , 'SPO2':b'%' \\\n",
    "             , 'RR':b'Imp' \\\n",
    "             , 'RESP':b'Imp' \\\n",
    "             , 'CVP1':b'cmH2O' \\\n",
    "             \n",
    "             }\n",
    "    scale = {'I': 2.44 \\\n",
    "             , 'II': 2.44 \\\n",
    "             , 'III': 2.44 \\\n",
    "             , 'V': 2.44 \\\n",
    "             , 'AVR': 2.44 \\\n",
    "             , 'AVL': 2.44 \\\n",
    "             , 'AVF': 2.44  \\\n",
    "             , 'AR1': 0.2 \\\n",
    "             , 'AR2': 0.2 \\\n",
    "             , 'AR3': 0.2 \\\n",
    "             , 'AR4': 0.2 \\\n",
    "             , 'AR5': 0.2 \\\n",
    "             , 'AR6': 0.2 \\\n",
    "             , 'AR7': 0.2 \\\n",
    "             , 'AR8': 0.2 \\\n",
    "             , 'SPO2': 1.0 \\\n",
    "             , 'RR': 0.1 \\\n",
    "             , 'RESP': 0.1 \\\n",
    "             , 'CVP1': 1.0 \\\n",
    "             }\n",
    "    offset = 0\n",
    "    range_high = 1\n",
    "    range_low = 0\n",
    "    # channel_data passed in dict\n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # Create units and scales fields from channel titles  passed in dict\n",
    "    ############################################################################\n",
    "    \n",
    "    # If the channel title being passed is not present in the units or scale\n",
    "    #  dictionaries, these statements will fail\n",
    "    \n",
    "    # Channel units\n",
    "    channel_units = []\n",
    "    for i in range(data_dict['num_channels']):\n",
    "        channel_units.append(units[data_dict['channel_titles'][i]])\n",
    "        \n",
    "    # Channel scales\n",
    "    channel_scales = []\n",
    "    for i in range(data_dict['num_channels']):\n",
    "        channel_scales.append(scale[data_dict['channel_titles'][i]])\n",
    "            \n",
    "   \n",
    "    ############################################################################\n",
    "    # Convert channel titles passed in dict to binary strings\n",
    "    ############################################################################\n",
    "    \n",
    "    # New list for binary channel titles\n",
    "    bin_channel_titles = []\n",
    "    for i in range(data_dict['num_channels']):\n",
    "        bin_channel_titles.append(\\\n",
    "                                data_dict['channel_titles'][i].encode('utf-8'))\n",
    "    \n",
    "    ############################################################################\n",
    "    # Append all adibin fields to list of lists\n",
    "    ############################################################################\n",
    "    \n",
    "    # Append file header    \n",
    "    adibin_header = []\n",
    "    \n",
    "    adibin_header.append(magic)\n",
    "    adibin_header.append(version)\n",
    "    adibin_header.append(secs_per_tick)\n",
    "    adibin_header.append(year)\n",
    "    adibin_header.append(month)\n",
    "    adibin_header.append(day)\n",
    "    adibin_header.append(hour)\n",
    "    adibin_header.append(minute)\n",
    "    adibin_header.append(second)\n",
    "    adibin_header.append(trigger)\n",
    "    adibin_header.append(data_dict['num_channels'])\n",
    "    adibin_header.append(data_dict['samples_per_channel'])\n",
    "    adibin_header.append(time_channel)\n",
    "    adibin_header.append(data_format)\n",
    " \n",
    "    # Append channel headers\n",
    "    adibin_channel_headers = []\n",
    "\n",
    "    for i in range(data_dict['num_channels']):\n",
    "        adibin_channel_headers.append(bin_channel_titles[i])\n",
    "        adibin_channel_headers.append(channel_units[i])\n",
    "        adibin_channel_headers.append(channel_scales[i])\n",
    "        adibin_channel_headers.append(offset)\n",
    "        adibin_channel_headers.append(range_high)\n",
    "        adibin_channel_headers.append(range_low)\n",
    "        \n",
    "    # Append interleaved channel data\n",
    "    adibin_channel_data = []\n",
    "    \n",
    "    for j in range(data_dict['samples_per_channel']):\n",
    "        for i in range(data_dict['num_channels']):\n",
    "            adibin_channel_data.append(data_dict['channel_data'][i][j])\n",
    "        \n",
    "    \n",
    "    \n",
    "    ############################################################################\n",
    "    # Pack adibin data into writable struct buffers\n",
    "    ############################################################################\n",
    "\n",
    "    ############################################################################\n",
    "    # Pack file header\n",
    "    # Create placeholder buffer of the right size\n",
    "    file_header_buffer = bytearray(FILE_HEADER_LENGTH)\n",
    "    \n",
    "    # Pack file header\n",
    "    struct.pack_into(ADI_FILE_HEADER_FORMAT_STRING\n",
    "                     , file_header_buffer\n",
    "                     , 0\n",
    "                     , *adibin_header\n",
    "                     )\n",
    "\n",
    "    ############################################################################\n",
    "    # Pack channel headers\n",
    "    # Create format string of right length for num_channels\n",
    "    channel_headers_format_string = ADI_CHANNEL_HEADER_FORMAT_STRING\n",
    "    for i in range(data_dict['num_channels'] - 1):\n",
    "        channel_headers_format_string += ADI_CHANNEL_HEADER_FORMAT_STRING[1:]\n",
    "    \n",
    "    # Create placeholder buffer of the right size\n",
    "    channel_headers_buffer = bytearray((data_dict['num_channels']) \\\n",
    "                                       * CHANNEL_HEADER_LENGTH)\n",
    "    \n",
    "    # Pack channel headers\n",
    "    struct.pack_into(channel_headers_format_string\n",
    "                     , channel_headers_buffer\n",
    "                     , 0\n",
    "                     , *adibin_channel_headers\n",
    "                     )\n",
    "\n",
    "    ############################################################################\n",
    "    # Pack channel data\n",
    "    # Create format string to write all the interwoven channel data\n",
    "    channel_data_format_string = ADI_CHANNEL_DATA_FORMAT_STRING\n",
    "    for i in range(\\\n",
    "        ((data_dict['num_channels'])*(data_dict['samples_per_channel']))- 1):\n",
    "        channel_data_format_string += ADI_CHANNEL_DATA_FORMAT_STRING[1:]\n",
    "    \n",
    "    # Create placeholder buffer of the right size\n",
    "    channel_data_buffer = bytearray(struct.calcsize(channel_data_format_string))\n",
    "    \n",
    "    # Pack channel data\n",
    "    struct.pack_into(channel_data_format_string\n",
    "                     , channel_data_buffer\n",
    "                     , 0\n",
    "                     , *adibin_channel_data\n",
    "                     )\n",
    "\n",
    "    \n",
    "    ############################################################################\n",
    "    # Debug: Check values\n",
    "    ############################################################################\n",
    "    if dbg == True:\n",
    "        print(\"channel_units:\", channel_units)\n",
    "        print(\"channel_scales:\", channel_scales)\n",
    "        print(\"adibin_data[0]:\", adibin_data[0])\n",
    "        print('\\n')\n",
    "        \n",
    "        \n",
    "    ############################################################################\n",
    "    # Return packed adibin buffers\n",
    "    ############################################################################\n",
    "    \n",
    "    # Create dictionary of packed buffers for return\n",
    "    adibin_data = {'alarm_id': data_dict['alarm_id']\n",
    "                  , 'time_since_admission': data_dict['time_since_admission']\n",
    "                  , 'file_header': file_header_buffer\n",
    "                  , 'channel_headers': channel_headers_buffer\n",
    "                  , 'channel_data': channel_data_buffer\n",
    "                  }\n",
    "    \n",
    "    return adibin_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write ADIBIN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def writeAdibin(adibin_data, csv_filename, output_directory, dbg=False):\n",
    "    \n",
    "    # Create output filename\n",
    "    filename = output_directory \\\n",
    "                + csv_filename \\\n",
    "                + \"_\" \\\n",
    "                + adibin_data['alarm_id'] \\\n",
    "                + \"_\" \\\n",
    "                + adibin_data['time_since_admission'] \\\n",
    "                + \".adibin\"\n",
    "    \n",
    "    # If directory does not exist, create it\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    \n",
    "    # Write the returned content to a file in the output directory\n",
    "    # overwrite previous file without warning\n",
    "    with open(filename, 'wb') as adibin_file:\n",
    "        adibin_file.write(adibin_data['file_header'])\n",
    "        adibin_file.write(adibin_data['channel_headers'])\n",
    "        adibin_file.write(adibin_data['channel_data'])\n",
    "\n",
    "    if dbg == True:\n",
    "        print(\"Writing:\", filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write CSV Row to SQLite Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csvRowToSqlite(data_dict, csv_filename, output_directory, dbg = False):\n",
    "    \n",
    "    # Add csv_filename to data_dict for ease\n",
    "    data_dict.update({'admission_id': csv_filename})\n",
    "    \n",
    "    # Open connection to SQLite database\n",
    "    #  or create database if it doesn't exist\n",
    "    connection = sqlite3.connect(output_directory + 'errorRows.db')\n",
    "    cursor = connection.cursor()\n",
    "    \n",
    "    # Create CSVROWS table if it doesn't exist\n",
    "    cursor.execute(\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS CSVROWS \n",
    "        (PRIMARY_KEY             INTEGER PRIMARY KEY\n",
    "        , ADMISSION_ID         VARCHAR(100)\n",
    "        , ALARM_ID             VARCHAR(100)\n",
    "        , TIME_SINCE_ADMISSION INTEGER\n",
    "        , NUM_CHANNELS         INTEGER\n",
    "        , CHANNEL_TITLES       SET\n",
    "        , CHANNEL_DATA         BLOB\n",
    "        , SAMPLES_PER_CHANNEL  INTEGER\n",
    "        )\n",
    "    ''')\n",
    "    \n",
    "    # Insert error row into CSVROWS table\n",
    "    cursor.execute(\n",
    "    '''\n",
    "    INSERT INTO CSVROWS\n",
    "        (PRIMARY_KEY\n",
    "        , ADMISSION_ID\n",
    "        , ALARM_ID\n",
    "        , TIME_SINCE_ADMISSION\n",
    "        , NUM_CHANNELS\n",
    "        , CHANNEL_TITLES\n",
    "        , CHANNEL_DATA\n",
    "        , SAMPLES_PER_CHANNEL\n",
    "        )\n",
    "    VALUES\n",
    "        (NULL\n",
    "        , :admission_id\n",
    "        , :alarm_id\n",
    "        , :time_since_admission\n",
    "        , :num_channels\n",
    "        , :channel_titles\n",
    "        , :channel_data\n",
    "        , :samples_per_channel\n",
    "        )\n",
    "    '''\n",
    "    , data_dict)\n",
    "    \n",
    "    # Debug\n",
    "    if dbg == True:\n",
    "        cursor.execute(\n",
    "        '''\n",
    "        SELECT *\n",
    "        /*\n",
    "            ADMISSION_ID\n",
    "            , ALARM_ID\n",
    "            , TIME_SINCE_ADMISSION\n",
    "        */\n",
    "        FROM CSVROWS\n",
    "        ''')\n",
    "        print(\"FETCHALL:\")\n",
    "        result = cursor.fetchall()\n",
    "        for r in result:\n",
    "            print(r)\n",
    "    \n",
    "    # Commit and close\n",
    "    connection.commit()\n",
    "    connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Bar Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printProgress(current_iteration, total_iterations, fill = '█'):\n",
    "    \n",
    "    # Default variables\n",
    "    bar_length = 70\n",
    "    \n",
    "    # Calculate percent complete\n",
    "    percent_complete = current_iteration / float(total_iterations)\n",
    "    \n",
    "    # Calculate filled length\n",
    "    filled_length = int(round(bar_length * percent_complete))\n",
    "    \n",
    "    # Create bar and message\n",
    "    percent_complete_msg = round(100.0 * percent_complete, 1)\n",
    "    progress_bar = fill * filled_length \\\n",
    "                 + '-' * (bar_length - filled_length)\n",
    "    \n",
    "    sys.stdout.write('\\r%s |%s| %s%% %s' % \\\n",
    "                     ('progress:'\n",
    "                      , progress_bar\n",
    "                      , percent_complete_msg\n",
    "                      , 'complete'\n",
    "                      )\n",
    "                    )\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nice CSV to ADIBIN Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csvToAdibin(csv_in_directory_path, adibin_out_directory_path, dbg=False):\n",
    "    \n",
    "    # Size of job for dbg progress bar\n",
    "    if dbg == True:\n",
    "        num_files_to_convert = \\\n",
    "            len([name for name in os.listdir(csv_in_directory_path)\\\n",
    "            if os.path.isfile(os.path.join(csv_in_directory_path, name))])\n",
    "        \n",
    "        current_iteration = 0\n",
    "        \n",
    "        printProgress(current_iteration, num_files_to_convert)\n",
    "        \n",
    "    \n",
    "    # For every csv file in the csv_in_directory_path\n",
    "    for csv_filename in glob.glob(csv_in_directory_path + '*.csv'):\n",
    "        \n",
    "        # Parse admission_id from csv_filename\n",
    "        csv_basename = os.path.basename(csv_filename)\n",
    "        admission_id = csv_basename[:-4]\n",
    "        \n",
    "        # Open CSV File\n",
    "        with open(csv_filename) as csv_file:\n",
    "            \n",
    "            # Parse and write out ADIBIN for every row in the CSV\n",
    "            try:\n",
    "                csv_file_reader = csv.reader(csv_file)\n",
    "                for row in csv_file_reader:\n",
    "                    writeAdibin(createAdibin(parseCsv(row, dbg=False), dbg=False)\n",
    "                                , admission_id\n",
    "                                , adibin_out_directory_path\n",
    "                                , dbg = False\n",
    "                                )\n",
    "            except:\n",
    "                # Create problemFile directory to catch problem files\n",
    "                os.makedirs(os.path.dirname(\"./problemFiles/\"), exist_ok=True)\n",
    "                os.rename(csv_filename\n",
    "                         , \"./problemFiles/\" + csv_basename)\n",
    "\n",
    "\n",
    "            if dbg == True:\n",
    "                current_iteration += 1\n",
    "                if current_iteration <= num_files_to_convert:\n",
    "                    printProgress(current_iteration, num_files_to_convert)\n",
    "                else:\n",
    "                    printProgress(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do It To It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_directory_path = \"./csvFiles/\"\n",
    "adibin_directory_path = \"./generatedAdibins/\"\n",
    "\n",
    "start_time = time.time()\n",
    "csvToAdibin(csv_directory_path, adibin_directory_path, dbg=True)\n",
    "end_time = time.time()\n",
    "printProgress(1,1)\n",
    "print(\"\\nran in %s seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress: |██████████████████████████████████████████████████████████████████████| 100.0% complete\n",
      "ran in 0.7001442909240723 seconds\n"
     ]
    }
   ],
   "source": [
    "#csv_file_name = \"./sampleFiles/4b80ff2eb1112815299d7a4e9a4a1957.csv\"\n",
    "csv_directory_path = \"./sampleFiles/\"\n",
    "adibin_directory_path = \"./testOutsAdibin/\"\n",
    "\n",
    "start_time = time.time()\n",
    "csvToAdibin(csv_directory_path, adibin_directory_path, dbg=True)\n",
    "end_time = time.time()\n",
    "printProgress(1,1)\n",
    "print(\"\\nran in %s seconds\" % (end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Notebook to Python Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook csvToAdibin.ipynb to python\n",
      "[NbConvertApp] Writing 16697 bytes to csvToAdibin.py\n"
     ]
    }
   ],
   "source": [
    "#!jupyter nbconvert --to=python csvToAdibin.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
